{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1855b133-8d17-45b9-82cd-333c7fdd1fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:1 BERTScore Comparison:\n",
      "      Word                                               Task  \\\n",
      "0  hushing  selecting or providing the correct definition ...   \n",
      "\n",
      "   Baseline_Avg_BERTScore  Generated_Avg_BERTScore  Difference  \n",
      "0                0.826615                      NaN         NaN  \n",
      "\n",
      "Comparison saved to Data/1to1_BERTScore_comparison.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bert_score import score\n",
    "\n",
    "def calculate_avg_bertscore_for_question(correct, distractors):\n",
    "    \"\"\"\n",
    "    Calculate the average BERTScore (F1) by comparing each distractor (as a string)\n",
    "    to the correct answer (as a string). If an input is missing, it is replaced with an empty string.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    correct = str(correct) if pd.notnull(correct) else \"\"\n",
    "    for distractor in distractors:\n",
    "        distractor = str(distractor) if pd.notnull(distractor) else \"\"\n",
    "        # Only compute score if both strings are non-empty\n",
    "        if correct and distractor:\n",
    "            P, R, F1 = score([distractor], [correct], lang='en', verbose=False)\n",
    "            scores.append(F1.mean().item())\n",
    "    return np.mean(scores) if scores else np.nan\n",
    "\n",
    "def compute_question_bertscore(row, suffix):\n",
    "    \"\"\"\n",
    "    Compute the average BERTScore for a question using columns with the given suffix.\n",
    "    Assumes columns: Choice_1, Choice_2, Choice_3 and Correct_Answer.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        correct_index = int(row[f'Correct_Answer{suffix}'])  # 1-indexed answer\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting correct answer: {e}\")\n",
    "        return np.nan\n",
    "    \n",
    "    correct_choice = row[f'Choice_{correct_index}{suffix}']\n",
    "    distractors = []\n",
    "    for i in range(1, 4):\n",
    "        if i != correct_index:\n",
    "            distractors.append(row[f'Choice_{i}{suffix}'])\n",
    "    return calculate_avg_bertscore_for_question(correct_choice, distractors)\n",
    "\n",
    "# ------------------------\n",
    "# Load the baseline and generated questions\n",
    "# ------------------------\n",
    "baseline_file = 'Data/baseline_morphology_questions.csv'  # New file saved from the generator's mc_data\n",
    "generated_file = 'Data/generated_morphology_questions.csv'\n",
    "\n",
    "baseline_df = pd.read_csv(baseline_file)\n",
    "generated_df = pd.read_csv(generated_file)\n",
    "\n",
    "# ------------------------\n",
    "# Standardize merge keys: Convert 'Word' and 'Task' to lowercase and strip whitespace\n",
    "# ------------------------\n",
    "for df in [baseline_df, generated_df]:\n",
    "    df['Word'] = df['Word'].astype(str).str.strip().str.lower()\n",
    "    df['Task'] = df['Task'].astype(str).str.strip().str.lower()\n",
    "\n",
    "# ------------------------\n",
    "# Merge the datasets on 'Word' and 'Task' for 1:1 comparison\n",
    "# ------------------------\n",
    "merged_df = pd.merge(generated_df, baseline_df, on=['Word', 'Task'], suffixes=('_gen', '_base'))\n",
    "\n",
    "if merged_df.empty:\n",
    "    print(\"No matching questions found between baseline and generated datasets. Check your merge keys!\")\n",
    "else:\n",
    "    # ------------------------\n",
    "    # Compute average BERTScore for each matched question in both sets\n",
    "    # ------------------------\n",
    "    merged_df['Baseline_Avg_BERTScore'] = merged_df.apply(lambda row: compute_question_bertscore(row, '_base'), axis=1)\n",
    "    merged_df['Generated_Avg_BERTScore'] = merged_df.apply(lambda row: compute_question_bertscore(row, '_gen'), axis=1)\n",
    "    \n",
    "    # Optional: compute the difference between generated and baseline scores\n",
    "    merged_df['Difference'] = merged_df['Generated_Avg_BERTScore'] - merged_df['Baseline_Avg_BERTScore']\n",
    "    \n",
    "    # ------------------------\n",
    "    # Save the 1:1 comparison to a new CSV file\n",
    "    # ------------------------\n",
    "    output_file = 'Data/1to1_BERTScore_comparison.csv'\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    \n",
    "    # Print a summary of the comparison\n",
    "    print(\"1:1 BERTScore Comparison:\")\n",
    "    print(merged_df[['Word', 'Task', 'Baseline_Avg_BERTScore', 'Generated_Avg_BERTScore', 'Difference']])\n",
    "    print(f\"\\nComparison saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e9f010-4a80-4e0e-9937-88fb8a0439c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
