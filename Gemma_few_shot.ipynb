{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d49c2d4-2123-4186-bf6c-308df0ffffbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading required libraries...\n",
      "PyTorch version: 2.2.2+cu121\n",
      "Libraries loaded in 1.76 seconds\n",
      "CUDA available: True\n",
      "GPU device: NVIDIA A100-SXM4-80GB\n",
      "GPU memory: 84.97 GB\n",
      "Loading data...\n",
      "Loaded 268 examples\n",
      "Training set: 214 examples\n",
      "Validation set: 54 examples\n",
      "Loading tokenizer and model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e474af607ab4d55a13d9516712a4603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating few-shot datasets...\n",
      "Created 214 few-shot training examples\n",
      "Created 54 few-shot training examples\n",
      "Creating dataloaders...\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/nlp/1.3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 0: loss = 21.2747\n",
      "  Batch 10: loss = 10.3585\n",
      "  Batch 20: loss = 2.5222\n",
      "  Batch 30: loss = 1.3865\n",
      "  Batch 40: loss = 2.2768\n",
      "  Batch 50: loss = 3.0543\n",
      "  Batch 60: loss = 2.1271\n",
      "  Batch 70: loss = 1.3417\n",
      "  Batch 80: loss = 0.7835\n",
      "  Batch 90: loss = 1.2772\n",
      "  Batch 100: loss = 2.9002\n",
      "  Batch 110: loss = 2.0829\n",
      "  Batch 120: loss = 3.5104\n",
      "  Batch 130: loss = 1.7551\n",
      "  Batch 140: loss = 1.6231\n",
      "  Batch 150: loss = 2.5616\n",
      "  Batch 160: loss = 3.5322\n",
      "  Batch 170: loss = 1.0038\n",
      "  Batch 180: loss = 0.7870\n",
      "  Batch 190: loss = 0.8998\n",
      "  Batch 200: loss = 2.6117\n",
      "  Batch 210: loss = 2.1571\n",
      "  Average training loss: 2.5146\n",
      "  Validation loss: 2.0888\n",
      "  Saving best model with validation loss: 2.0888\n",
      "\n",
      "Epoch 2/3\n",
      "  Batch 0: loss = 2.0619\n",
      "  Batch 10: loss = 0.8950\n",
      "  Batch 20: loss = 0.5877\n",
      "  Batch 30: loss = 1.6916\n",
      "  Batch 40: loss = 3.6034\n",
      "  Batch 50: loss = 2.0802\n",
      "  Batch 60: loss = 1.7429\n",
      "  Batch 70: loss = 0.8264\n",
      "  Batch 80: loss = 3.0027\n",
      "  Batch 90: loss = 1.0799\n",
      "  Batch 100: loss = 0.4487\n",
      "  Batch 110: loss = 1.8701\n",
      "  Batch 120: loss = 0.9308\n",
      "  Batch 130: loss = 2.7827\n",
      "  Batch 140: loss = 0.7957\n",
      "  Batch 150: loss = 3.5833\n",
      "  Batch 160: loss = 1.0836\n",
      "  Batch 170: loss = 2.6400\n",
      "  Batch 180: loss = 1.3054\n",
      "  Batch 190: loss = 1.3122\n",
      "  Batch 200: loss = 2.6678\n",
      "  Batch 210: loss = 1.4184\n",
      "  Average training loss: 1.5170\n",
      "  Validation loss: 2.3432\n",
      "\n",
      "Epoch 3/3\n",
      "  Batch 0: loss = 0.2658\n",
      "  Batch 10: loss = 5.7477\n",
      "  Batch 20: loss = 1.9934\n",
      "  Batch 30: loss = 1.1059\n",
      "  Batch 40: loss = 1.5938\n",
      "  Batch 50: loss = 0.3270\n",
      "  Batch 60: loss = 1.0632\n",
      "  Batch 70: loss = 4.9423\n",
      "  Batch 80: loss = 0.3637\n",
      "  Batch 90: loss = 2.3043\n",
      "  Batch 100: loss = 3.1149\n",
      "  Batch 110: loss = 1.0583\n",
      "  Batch 120: loss = 3.3094\n",
      "  Batch 130: loss = 1.4978\n",
      "  Batch 140: loss = 0.5540\n",
      "  Batch 150: loss = 0.4677\n",
      "  Batch 160: loss = 1.3798\n",
      "  Batch 170: loss = 4.3893\n",
      "  Batch 180: loss = 0.5070\n",
      "  Batch 190: loss = 2.0455\n",
      "  Batch 200: loss = 1.8515\n",
      "  Batch 210: loss = 0.2182\n",
      "  Average training loss: 1.5041\n",
      "  Validation loss: 2.6605\n",
      "Training completed. Final model saved to gemma_few_shot_20250327_162345/final_model\n",
      "Training completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get token from environment\n",
    "HF_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Please set the HUGGINGFACE_TOKEN environment variable\")\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "# Dataset class for few-shot approach\n",
    "class MorphologyFewShotDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer=None, max_length=512, num_examples=3):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.num_examples = num_examples\n",
    "        self.examples = []\n",
    "        \n",
    "        for _, row in data.iterrows():\n",
    "            try:\n",
    "                # Get few-shot examples\n",
    "                few_shot_examples = self.get_few_shot_examples(row)\n",
    "                \n",
    "                # Create prompt with few-shot examples\n",
    "                prompt = f\"Question about morphology. Here are some examples:\\n\\n\"\n",
    "                prompt += few_shot_examples\n",
    "                prompt += f\"\\nNow answer this question:\\n\"\n",
    "                prompt += f\"Task: {row.get('Task', 'Identify')}\\n\"\n",
    "                prompt += f\"Word: {row.get('Word', '')}\\n\"\n",
    "                prompt += f\"Question: {row.get('Instruction', '')}\\n\"\n",
    "                \n",
    "                # Add choices if available\n",
    "                if pd.notna(row.get('Choice_1', pd.NA)):\n",
    "                    prompt += \"Choices:\\n\"\n",
    "                    choice_num = 1\n",
    "                    while True:\n",
    "                        choice_key = f'Choice_{choice_num}'\n",
    "                        if choice_key not in row or pd.isna(row[choice_key]):\n",
    "                            break\n",
    "                        prompt += f\"{choice_num}. {row[choice_key]}\\n\"\n",
    "                        choice_num += 1\n",
    "                \n",
    "                # Create expected output\n",
    "                if pd.notna(row.get('Correct_Answer', pd.NA)):\n",
    "                    correct_answer = str(row['Correct_Answer'])\n",
    "                    answer = f\"Answer: Choice {correct_answer}\"\n",
    "                else:\n",
    "                    answer = \"Answer: Unable to determine\"\n",
    "                \n",
    "                # Store example\n",
    "                self.examples.append({\n",
    "                    'prompt': prompt,\n",
    "                    'answer': answer\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row: {e}\")\n",
    "        \n",
    "        print(f\"Created {len(self.examples)} few-shot training examples\")\n",
    "    \n",
    "    def get_few_shot_examples(self, current_row):\n",
    "        \"\"\"Get few-shot examples similar to the current question\"\"\"\n",
    "        # Try to find similar examples (same category and task)\n",
    "        similar_rows = self.data[\n",
    "            (self.data['Category'] == current_row.get('Category', '')) & \n",
    "            (self.data['Task'] == current_row.get('Task', '')) &\n",
    "            (self.data.index != current_row.name)  # Exclude current row\n",
    "        ]\n",
    "        \n",
    "        # If not enough, try just the same category\n",
    "        if len(similar_rows) < self.num_examples:\n",
    "            similar_rows = self.data[\n",
    "                (self.data['Category'] == current_row.get('Category', '')) &\n",
    "                (self.data.index != current_row.name)\n",
    "            ]\n",
    "        \n",
    "        # If still not enough, use random examples\n",
    "        if len(similar_rows) < self.num_examples:\n",
    "            similar_rows = self.data[self.data.index != current_row.name]\n",
    "        \n",
    "        # Sample examples\n",
    "        if len(similar_rows) <= self.num_examples:\n",
    "            example_rows = similar_rows\n",
    "        else:\n",
    "            example_rows = similar_rows.sample(n=self.num_examples)\n",
    "        \n",
    "        # Format examples\n",
    "        examples_text = \"\"\n",
    "        for i, (_, example) in enumerate(example_rows.iterrows(), 1):\n",
    "            examples_text += f\"Example {i}:\\n\"\n",
    "            examples_text += f\"Task: {example.get('Task', 'Identify')}\\n\"\n",
    "            examples_text += f\"Word: {example.get('Word', '')}\\n\"\n",
    "            examples_text += f\"Question: {example.get('Instruction', '')}\\n\"\n",
    "            \n",
    "            if pd.notna(example.get('Choice_1', pd.NA)):\n",
    "                examples_text += \"Choices:\\n\"\n",
    "                choice_num = 1\n",
    "                while True:\n",
    "                    choice_key = f'Choice_{choice_num}'\n",
    "                    if choice_key not in example or pd.isna(example[choice_key]):\n",
    "                        break\n",
    "                    examples_text += f\"{choice_num}. {example[choice_key]}\\n\"\n",
    "                    choice_num += 1\n",
    "            \n",
    "            if pd.notna(example.get('Correct_Answer', pd.NA)):\n",
    "                correct_answer = str(example['Correct_Answer'])\n",
    "                examples_text += f\"Answer: Choice {correct_answer}\\n\\n\"\n",
    "            else:\n",
    "                examples_text += f\"Answer: {example.get('Correct_Answer', 'Unknown')}\\n\\n\"\n",
    "        \n",
    "        return examples_text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "# Simple tokenization function\n",
    "def tokenize_data(examples, tokenizer, max_length=512):\n",
    "    \"\"\"Tokenize a batch of examples\"\"\"\n",
    "    prompts = [ex['prompt'] for ex in examples]\n",
    "    answers = [ex['answer'] for ex in examples]\n",
    "    \n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(\n",
    "        prompts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        targets = tokenizer(\n",
    "            answers,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    # Create input_ids and labels\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "    labels = targets.input_ids\n",
    "    \n",
    "    # Replace padding token id with -100 so it's ignored in loss\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "# Basic training function\n",
    "def train_model(train_dataloader, val_dataloader, model, tokenizer, \n",
    "               num_epochs=3, learning_rate=5e-5, output_dir=\"gemma_few_shot\"):\n",
    "    \"\"\"Train the model with a basic training loop\"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    global_step = 0\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        # Training\n",
    "        for batch_idx, batch in enumerate(train_dataloader):\n",
    "            # Prepare batch\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "            )\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            global_step += 1\n",
    "            \n",
    "            # Log progress\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"  Batch {batch_idx}: loss = {loss.item():.4f}\")\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_train_loss = epoch_loss / len(train_dataloader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "        print(f\"  Average training loss: {avg_train_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "                \n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "                \n",
    "                val_loss += outputs.loss.item()\n",
    "        \n",
    "        # Calculate average validation loss\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        print(f\"  Validation loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Save model if it's the best so far\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            print(f\"  Saving best model with validation loss: {best_val_loss:.4f}\")\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(output_dir, \"best_model\")\n",
    "            os.makedirs(model_path, exist_ok=True)\n",
    "            \n",
    "            # Save model state dict\n",
    "            torch.save(model.state_dict(), os.path.join(model_path, \"pytorch_model.bin\"))\n",
    "            \n",
    "            # Save config\n",
    "            model_config = model.config.to_dict()\n",
    "            with open(os.path.join(model_path, \"config.json\"), 'w') as f:\n",
    "                json.dump(model_config, f)\n",
    "            \n",
    "            # Save tokenizer\n",
    "            tokenizer.save_pretrained(model_path)\n",
    "    \n",
    "    # Save final model\n",
    "    final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "    os.makedirs(final_model_path, exist_ok=True)\n",
    "    \n",
    "    # Save model state dict\n",
    "    torch.save(model.state_dict(), os.path.join(final_model_path, \"pytorch_model.bin\"))\n",
    "    \n",
    "    # Save config\n",
    "    model_config = model.config.to_dict()\n",
    "    with open(os.path.join(final_model_path, \"config.json\"), 'w') as f:\n",
    "        json.dump(model_config, f)\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(final_model_path)\n",
    "    \n",
    "    # Save training stats\n",
    "    training_stats = {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"val_losses\": val_losses,\n",
    "        \"best_val_loss\": best_val_loss\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(output_dir, \"training_stats.json\"), 'w') as f:\n",
    "        json.dump(training_stats, f)\n",
    "    \n",
    "    print(f\"Training completed. Final model saved to {final_model_path}\")\n",
    "    return model, tokenizer\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    try:\n",
    "        # Load libraries dynamically to avoid import errors\n",
    "        print(\"Loading required libraries...\")\n",
    "        import_start = time.time()\n",
    "        \n",
    "        # First try to import a minimal PyTorch\n",
    "        import torch\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        \n",
    "        # Then try to import tokenizer and model classes directly\n",
    "        from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "        \n",
    "        import_end = time.time()\n",
    "        print(f\"Libraries loaded in {import_end - import_start:.2f} seconds\")\n",
    "        \n",
    "        # Set paths and parameters\n",
    "        data_path = 'Data/MC_data_MMA.csv'\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_dir = f\"gemma_few_shot_{timestamp}\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Display GPU information\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "            print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "        \n",
    "        # Load data\n",
    "        print(\"Loading data...\")\n",
    "        df = pd.read_csv(data_path)\n",
    "        print(f\"Loaded {len(df)} examples\")\n",
    "        \n",
    "        # Split data\n",
    "        train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        print(f\"Training set: {len(train_df)} examples\")\n",
    "        print(f\"Validation set: {len(val_df)} examples\")\n",
    "        \n",
    "        # Load tokenizer and model\n",
    "        print(\"Loading tokenizer and model...\")\n",
    "        model_name = \"google/gemma-2b-it\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name, token=HF_TOKEN)\n",
    "        \n",
    "        # Create datasets\n",
    "        print(\"Creating few-shot datasets...\")\n",
    "        train_dataset = MorphologyFewShotDataset(train_df, tokenizer, num_examples=3)\n",
    "        val_dataset = MorphologyFewShotDataset(val_df, tokenizer, num_examples=3)\n",
    "        \n",
    "        # Create dataloaders\n",
    "        print(\"Creating dataloaders...\")\n",
    "        \n",
    "        # Collate function\n",
    "        def collate_fn(batch):\n",
    "            return tokenize_data(batch, tokenizer)\n",
    "        \n",
    "        # Create dataloaders\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=1,  # Small batch size due to model size\n",
    "            shuffle=True, \n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=1, \n",
    "            shuffle=False, \n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        # Train model\n",
    "        print(\"Starting training...\")\n",
    "        model, tokenizer = train_model(\n",
    "            train_dataloader,\n",
    "            val_dataloader,\n",
    "            model,\n",
    "            tokenizer,\n",
    "            num_epochs=3,\n",
    "            learning_rate=5e-5,\n",
    "            output_dir=output_dir\n",
    "        )\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4480e4e7-9032-4426-8924-6bae89c034e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-1.3",
   "language": "python",
   "name": "nlp-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
