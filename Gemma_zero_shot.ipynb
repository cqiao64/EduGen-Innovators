{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee40a539-925b-4201-a9c7-bb742dccfc40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU device: NVIDIA A100-SXM4-80GB\n",
      "Total GPU memory: 79.14 GB\n",
      "\n",
      "Preparing data...\n",
      "Original data size: 268\n",
      "Training set size: 227, Validation set size: 41\n",
      "\n",
      "Training Gemma model with zero-shot approach...\n",
      "Initial GPU state: Allocated: 0.00 GB, Max: 0.00 GB, Reserved: 0.00 GB\n",
      "Config saved locally to gemma_zero_shot_20250327_162909/base_model_config\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "948eb0c3884a47e89b67bf08bdfd6596",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After model loading: Allocated: 5.64 GB, Max: 5.64 GB, Reserved: 5.65 GB\n",
      "Applying LoRA adapters...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,686,400 || all params: 2,509,858,816 || trainable%: 0.14687678751090355\n",
      "After model adaptation: Allocated: 5.65 GB, Max: 5.65 GB, Reserved: 5.65 GB\n",
      "Creating zero-shot datasets...\n",
      "Created 227 zero-shot training examples\n",
      "Created 41 zero-shot training examples\n",
      "After dataset creation: Allocated: 5.65 GB, Max: 5.65 GB, Reserved: 5.65 GB\n",
      "Before trainer initialization: Allocated: 5.65 GB, Max: 5.65 GB, Reserved: 5.65 GB\n",
      "After trainer initialization: Allocated: 5.65 GB, Max: 5.65 GB, Reserved: 5.65 GB\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 22:46, Epoch 11/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.514400</td>\n",
       "      <td>3.152705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.667100</td>\n",
       "      <td>1.447751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.636500</td>\n",
       "      <td>0.570354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.346100</td>\n",
       "      <td>0.350446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.279000</td>\n",
       "      <td>0.312200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.264100</td>\n",
       "      <td>0.295596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.259200</td>\n",
       "      <td>0.284656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.260400</td>\n",
       "      <td>0.280846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.248200</td>\n",
       "      <td>0.277584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.239400</td>\n",
       "      <td>0.276542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.241400</td>\n",
       "      <td>0.276165</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved to gemma_zero_shot_20250327_162909/final_model\n",
      "\n",
      "Testing model on sample questions...\n",
      "\n",
      "Sample Generated Answers:\n",
      "\n",
      "Example 1:\n",
      "Question: What is the base word and suffix in 'happiness'?\n",
      "Word: happiness\n",
      "Generated Answer: Answer: Choice 1. base: happy, suffix: -ness is correct because it demonstrates the concept of derivation through proper identification of the key words in the vocabulary/word form. This is crucial because it</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Question: Identify the type of compound word in 'blackboard'.\n",
      "Word: blackboard\n",
      "Generated Answer: Answer: Choice 2. Exocentric compound. In the 'blackboard' word, we can identify the types of compounds through analysis is correct because it demonstrates the concept of exocentric compound. This is because in the 'blackboard' word, we can trace the relationship between words to show that it has two partsâ€”the prefix/root word black and the suffix/morpheme board. This tracing process shows us an example of an exocentric compound.</s>\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get token from environment\n",
    "HF_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Please set the HUGGINGFACE_TOKEN environment variable\")\n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "class MorphologyZeroShotDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.examples = []\n",
    "        \n",
    "        # Process each row in the dataset\n",
    "        for _, row in data.iterrows():\n",
    "            # Create zero-shot prompt (direct question without examples)\n",
    "            input_text = self.create_zero_shot_prompt(row)\n",
    "            \n",
    "            # Create expected output\n",
    "            target_text = self.create_target_output(row)\n",
    "            \n",
    "            # Combine for full text\n",
    "            full_text = f\"{input_text}{target_text}</s>\"\n",
    "            \n",
    "            # Store the example\n",
    "            self.examples.append(full_text)\n",
    "            \n",
    "        print(f\"Created {len(self.examples)} zero-shot training examples\")\n",
    "    \n",
    "    def create_zero_shot_prompt(self, row):\n",
    "        \"\"\"Create a prompt without any examples (zero-shot)\"\"\"\n",
    "        # Main instruction\n",
    "        instruction = (\n",
    "            \"You are answering a morphology question. \"\n",
    "            \"Begin your answer with 'Answer: Choice X' for multiple choice or 'Answer: [text]' for open-ended questions. \"\n",
    "            \"Provide a clear explanation afterward.\"\n",
    "        )\n",
    "        \n",
    "        # Determine if it's multiple choice\n",
    "        task_type = \"Multiple choice\" if pd.notna(row.get('Choice_1', pd.NA)) else \"Open-ended\"\n",
    "        \n",
    "        # Format the input\n",
    "        input_text = (\n",
    "            f\"{instruction}\\n\\n\"\n",
    "            f\"# Question Information\\n\"\n",
    "            f\"- Type: {task_type}\\n\"\n",
    "            f\"- Task: {row['Task']}\\n\"\n",
    "            f\"- Category: {row['Category']}\\n\"\n",
    "            f\"- Word: {row['Word']}\\n\"\n",
    "            f\"- Question: {row['Instruction']}\\n\"\n",
    "        )\n",
    "        \n",
    "        # Add choices if multiple choice\n",
    "        if pd.notna(row.get('Choice_1', pd.NA)):\n",
    "            input_text += \"\\n# Available Choices\\n\"\n",
    "            choice_num = 1\n",
    "            while True:\n",
    "                choice_key = f'Choice_{choice_num}'\n",
    "                if choice_key not in row or pd.isna(row[choice_key]):\n",
    "                    break\n",
    "                input_text += f\"- Choice {choice_num}: {row[choice_key]}\\n\"\n",
    "                choice_num += 1\n",
    "        \n",
    "        # Add separator\n",
    "        input_text += \"\\n# Your Answer:\\n\"\n",
    "        \n",
    "        return input_text\n",
    "    \n",
    "    def create_target_output(self, row):\n",
    "        \"\"\"Create the expected model output\"\"\"\n",
    "        if pd.notna(row.get('Choice_1', pd.NA)):\n",
    "            correct_choice = row[f'Choice_{row[\"Correct_Answer\"]}']\n",
    "            target_text = (\n",
    "                f\"Answer: Choice {row['Correct_Answer']}. \"\n",
    "                f\"{correct_choice} is correct because it demonstrates the {row['Category'].lower()} \"\n",
    "                f\"concept. In the word '{row['Word']}', we can identify the {row['Task'].lower()} \"\n",
    "                f\"through proper morphological analysis. This is a key concept in understanding \"\n",
    "                f\"how words are formed and structured in English.\"\n",
    "            )\n",
    "        else:\n",
    "            target_text = (\n",
    "                f\"Answer: {str(row['Correct_Answer'])}. \"\n",
    "                f\"This demonstrates the {row['Category'].lower()} concept in '{row['Word']}'. \"\n",
    "                f\"When analyzing how this word {row['Task'].lower()}, we can see the morphological \"\n",
    "                f\"principles at work. This helps us understand the structure and formation of words.\"\n",
    "            )\n",
    "        \n",
    "        return target_text\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.examples[idx]\n",
    "        \n",
    "        # Tokenize the text\n",
    "        encodings = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Separate the input part and create labels\n",
    "        input_part = self.tokenizer(\n",
    "            text.split(\"# Your Answer:\")[0] + \"# Your Answer:\\n\",\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Get length of the input part\n",
    "        input_length = input_part['input_ids'].shape[1]\n",
    "        \n",
    "        # Create labels, masking the input part\n",
    "        labels = encodings['input_ids'].clone()\n",
    "        labels[:, :input_length] = -100  # Ignore loss for the input part\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        }\n",
    "\n",
    "def prepare_data(csv_path):\n",
    "    \"\"\"Load and prepare the data for training\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert necessary columns to string\n",
    "    for col in ['Correct_Answer', 'Word_Difficulty', 'Task_Difficulty']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Split data with stratification if possible\n",
    "    try:\n",
    "        train_df, val_df = train_test_split(\n",
    "            df, \n",
    "            test_size=0.15,\n",
    "            random_state=42,\n",
    "            stratify=df[['Category', 'Task']].apply(lambda x: f\"{x['Category']}_{x['Task']}\", axis=1)\n",
    "        )\n",
    "    except ValueError:\n",
    "        # Fallback to stratifying by just Category\n",
    "        train_df, val_df = train_test_split(\n",
    "            df, \n",
    "            test_size=0.15, \n",
    "            random_state=42, \n",
    "            stratify=df['Category']\n",
    "        )\n",
    "    \n",
    "    print(f\"Original data size: {len(df)}\")\n",
    "    print(f\"Training set size: {len(train_df)}, Validation set size: {len(val_df)}\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "def monitor_gpu_memory(message):\n",
    "    \"\"\"Helper function to monitor GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        print(f\"{message}: Allocated: {allocated:.2f} GB, Max: {max_allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "def save_config_locally(model_name, output_dir):\n",
    "    \"\"\"Save model config locally to avoid authentication issues during training\"\"\"\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    import shutil\n",
    "    \n",
    "    # Create directory structure\n",
    "    base_path = os.path.join(output_dir, \"base_model_config\")\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Download config file\n",
    "        config_path = hf_hub_download(\n",
    "            repo_id=model_name,\n",
    "            filename=\"config.json\",\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "        \n",
    "        # Copy to our directory\n",
    "        shutil.copy(config_path, os.path.join(base_path, \"config.json\"))\n",
    "        print(f\"Config saved locally to {base_path}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving config locally: {e}\")\n",
    "        return False\n",
    "\n",
    "def train_zero_shot_gemma(train_df, val_df, model_name=\"google/gemma-2b-it\", output_dir=\"gemma_zero_shot\", use_lora=True):\n",
    "    \"\"\"Fine-tune Gemma model with zero-shot approach\"\"\"\n",
    "    # Clean memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    monitor_gpu_memory(\"Initial GPU state\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save config locally to avoid authentication issues\n",
    "    save_config_locally(model_name, output_dir)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "    \n",
    "    # Save tokenizer locally\n",
    "    tokenizer_save_path = os.path.join(output_dir, \"tokenizer\")\n",
    "    tokenizer.save_pretrained(tokenizer_save_path)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=HF_TOKEN,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float32,  # Use float32 to avoid gradient issues\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    monitor_gpu_memory(\"After model loading\")\n",
    "    \n",
    "    # Apply LoRA for efficient fine-tuning if requested\n",
    "    if use_lora:\n",
    "        print(\"Applying LoRA adapters...\")\n",
    "        # Configure LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,  # rank\n",
    "            lora_alpha=32,  # scaling factor\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        \n",
    "        # Apply LoRA to model\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        # Enable gradient checkpointing for full fine-tuning\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    \n",
    "    monitor_gpu_memory(\"After model adaptation\")\n",
    "    \n",
    "    # Create zero-shot datasets\n",
    "    print(\"Creating zero-shot datasets...\")\n",
    "    train_dataset = MorphologyZeroShotDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = MorphologyZeroShotDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    monitor_gpu_memory(\"After dataset creation\")\n",
    "    \n",
    "    # Training arguments\n",
    "    batch_size = 2 if use_lora else 1\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=12,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=3e-5 if use_lora else 2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=30,\n",
    "        save_steps=30,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        gradient_accumulation_steps=4 if use_lora else 8,\n",
    "        fp16=False,  # Disable fp16 to avoid gradient issues\n",
    "        bf16=False,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        optim=\"adamw_torch\",\n",
    "        gradient_checkpointing=not use_lora,  # Enable for full fine-tuning only\n",
    "        ddp_find_unused_parameters=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",  # Disable reporting to save memory\n",
    "        run_name=f\"gemma_zero_shot_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        hub_token=HF_TOKEN,\n",
    "    )\n",
    "    \n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    monitor_gpu_memory(\"Before trainer initialization\")\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "    \n",
    "    monitor_gpu_memory(\"After trainer initialization\")\n",
    "    \n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    model_save_path = os.path.join(output_dir, \"final_model\")\n",
    "    \n",
    "    if use_lora:\n",
    "        # For LoRA, save the adapter\n",
    "        model.save_pretrained(model_save_path, token=HF_TOKEN)\n",
    "    else:\n",
    "        # For full model, save everything\n",
    "        trainer.save_model(model_save_path)\n",
    "    \n",
    "    # Save tokenizer from local copy\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def test_model(model, tokenizer, test_questions):\n",
    "    \"\"\"Test the model on sample questions\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for question in test_questions:\n",
    "        # Create zero-shot prompt\n",
    "        instruction = (\n",
    "            \"You are answering a morphology question. \"\n",
    "            \"Begin your answer with 'Answer: Choice X' for multiple choice or 'Answer: [text]' for open-ended questions. \"\n",
    "            \"Provide a clear explanation afterward.\"\n",
    "        )\n",
    "        \n",
    "        # Format input\n",
    "        input_text = (\n",
    "            f\"{instruction}\\n\\n\"\n",
    "            f\"# Question Information\\n\"\n",
    "            f\"- Type: Multiple choice\\n\"\n",
    "            f\"- Task: {question['Task']}\\n\"\n",
    "            f\"- Category: {question['Category']}\\n\"\n",
    "            f\"- Word: {question['Word']}\\n\"\n",
    "            f\"- Question: {question['Instruction']}\\n\"\n",
    "            f\"\\n# Available Choices\\n\"\n",
    "        )\n",
    "        \n",
    "        # Add choices\n",
    "        for i, choice in enumerate(question['Choices'], 1):\n",
    "            input_text += f\"- Choice {i}: {choice}\\n\"\n",
    "        \n",
    "        # Add answer prompt\n",
    "        input_text += \"\\n# Your Answer:\\n\"\n",
    "        \n",
    "        # Generation settings\n",
    "        generation_config = {\n",
    "            'max_new_tokens': 200,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.92,\n",
    "            'top_k': 50,\n",
    "            'repetition_penalty': 1.2,\n",
    "        }\n",
    "        \n",
    "        # Generate answer\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                **generation_config\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer part\n",
    "        if input_text in generated_text:\n",
    "            answer = generated_text[len(input_text):].strip()\n",
    "        else:\n",
    "            answer = generated_text\n",
    "            \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'input': input_text,\n",
    "            'generated': answer\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Configure paths\n",
    "    data_path = 'Data/MC_data_MMA.csv'\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = f\"gemma_zero_shot_{timestamp}\"\n",
    "    \n",
    "    # Display GPU information\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nPreparing data...\")\n",
    "    train_df, val_df = prepare_data(data_path)\n",
    "    \n",
    "    # Define whether to use LoRA (recommended for better memory efficiency)\n",
    "    use_lora = True  # Set to False for full fine-tuning\n",
    "    \n",
    "    # Train model with zero-shot approach\n",
    "    print(f\"\\nTraining Gemma model with zero-shot approach...\")\n",
    "    model, tokenizer = train_zero_shot_gemma(train_df, val_df, output_dir=output_dir, use_lora=use_lora)\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        {\n",
    "            \"Task\": \"Identify\",\n",
    "            \"Category\": \"Derivation\",\n",
    "            \"Word\": \"happiness\",\n",
    "            \"Instruction\": \"What is the base word and suffix in 'happiness'?\",\n",
    "            \"Choices\": [\n",
    "                \"base: happy, suffix: -ness\",\n",
    "                \"base: happ, suffix: -iness\",\n",
    "                \"base: happi, suffix: -ness\",\n",
    "                \"base: hap, suffix: -piness\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Task\": \"Analyze\",\n",
    "            \"Category\": \"Compounding\",\n",
    "            \"Word\": \"blackboard\",\n",
    "            \"Instruction\": \"Identify the type of compound word in 'blackboard'.\",\n",
    "            \"Choices\": [\n",
    "                \"Endocentric compound\",\n",
    "                \"Exocentric compound\",\n",
    "                \"Copulative compound\",\n",
    "                \"Appositional compound\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"\\nTesting model on sample questions...\")\n",
    "    results = test_model(model, tokenizer, test_questions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nSample Generated Answers:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Question: {result['question']['Instruction']}\")\n",
    "        print(f\"Word: {result['question']['Word']}\")\n",
    "        print(f\"Generated Answer: {result['generated']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clean memory before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9b0e03-41ce-4d20-922f-a38f2f146678",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-1.3",
   "language": "python",
   "name": "nlp-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
