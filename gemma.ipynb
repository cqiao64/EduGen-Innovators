{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b905b88-7f48-4bce-a1e6-9ff10392547a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU device: NVIDIA A100-SXM4-80GB\n",
      "Total GPU memory: 79.14 GB\n",
      "\n",
      "Preparing data...\n",
      "Original data size: 268\n",
      "Augmented data size: 268\n",
      "Training set size: 227, Validation set size: 41\n",
      "\n",
      "Training Gemma model with LoRA...\n",
      "Initial GPU state: Allocated: 8.50 GB, Max: 18.44 GB, Reserved: 8.64 GB\n",
      "Config saved locally to gemma_morphology_20250327_001941/base_model_config\n",
      "Loading tokenizer...\n",
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff4675b31ce4221a4e3d1ca4595ba5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After model loading: Allocated: 14.15 GB, Max: 18.44 GB, Reserved: 14.17 GB\n",
      "Applying LoRA adapters...\n",
      "trainable params: 3,686,400 || all params: 2,509,858,816 || trainable%: 0.14687678751090355\n",
      "After model adaptation: Allocated: 14.16 GB, Max: 18.44 GB, Reserved: 14.17 GB\n",
      "Creating datasets...\n",
      "After dataset creation: Allocated: 14.16 GB, Max: 18.44 GB, Reserved: 14.17 GB\n",
      "Before trainer initialization: Allocated: 14.16 GB, Max: 18.44 GB, Reserved: 14.17 GB\n",
      "After trainer initialization: Allocated: 14.16 GB, Max: 18.44 GB, Reserved: 14.17 GB\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='336' max='336' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [336/336 08:33, Epoch 11/12]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.519000</td>\n",
       "      <td>3.137833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.693800</td>\n",
       "      <td>1.447343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.649100</td>\n",
       "      <td>0.552169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.358300</td>\n",
       "      <td>0.351613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.285700</td>\n",
       "      <td>0.312155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.256900</td>\n",
       "      <td>0.295255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.265100</td>\n",
       "      <td>0.288292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.285424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.235800</td>\n",
       "      <td>0.281910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.240400</td>\n",
       "      <td>0.280709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.236200</td>\n",
       "      <td>0.280437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Model saved to gemma_morphology_20250327_001941/final_model\n",
      "\n",
      "Testing model on sample questions...\n",
      "\n",
      "Sample Generated Answers:\n",
      "\n",
      "Example 1:\n",
      "Question: What is the base word and suffix in 'happiness'?\n",
      "Word: happiness\n",
      "Generated Answer: Answer: Choice 1. base: happy, suffix: -ness is correct because it demonstrates the concept of derivation through identification. In the word 'happiness', we can identify the base word and suffix through proper morphological analysis. This is a key concept in understanding how words are formed and structured in English.</s>\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "Question: Identify the type of compound word in 'blackboard'.\n",
      "Word: blackboard\n",
      "Generated Answer: Answer: Choice 3. Copulative compound is correct because it demonstrates the concept in the question. It shows that the morph/copy is available in the word through proper morphological analysis. This is what makes the word meaningful and allows us to understand its meaning.</s>\n",
      "\n",
      "In the word 'blackboard', we can identify the concept through proper morphological analysis. This is because we can demonstrate that the morph/copy is available in the word through proper morphological analysis. This is what makes the word meaningful and allows us to understand its meaning.\n",
      "\n",
      "# Additional Information\n",
      "- The word has three words: endocentric compound\n",
      "- The word has a morpheme commonality between the first two words\n",
      "- The word has a morpheme commonality between the second and third words\n",
      "\n",
      "# Conclusion\n",
      "The word 'blackboard' is an example of a copulative compound because it demonstrates the concept in the question. It shows that the morph/copy is available in the word through proper morphological analysis. This is what\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get token from environment\n",
    "HF_TOKEN = os.environ.get(\"HUGGINGFACE_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    raise ValueError(\"Please set the HUGGINGFACE_TOKEN environment variable\")\n",
    "                     \n",
    "os.environ[\"HF_TOKEN\"] = HF_TOKEN  # Set environment variable for token\n",
    "\n",
    "class MorphologyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length=512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        \n",
    "        # Structured prompt format\n",
    "        instruction = (\n",
    "            \"You are answering a morphology question. \"\n",
    "            \"Begin your answer with 'Answer: Choice X' for multiple choice or 'Answer: [text]' for open-ended questions. \"\n",
    "            \"Provide a clear explanation afterward.\"\n",
    "        )\n",
    "        \n",
    "        task_type = \"Multiple choice\" if pd.notna(item['Choice_1']) else \"Open-ended\"\n",
    "        \n",
    "        # Input format with clear separation\n",
    "        input_text = (\n",
    "            f\"{instruction}\\n\\n\"\n",
    "            f\"# Question Information\\n\"\n",
    "            f\"- Type: {task_type}\\n\"\n",
    "            f\"- Task: {item['Task']}\\n\"\n",
    "            f\"- Category: {item['Category']}\\n\"\n",
    "            f\"- Word: {item['Word']}\\n\"\n",
    "            f\"- Question: {item['Instruction']}\\n\"\n",
    "        )\n",
    "        \n",
    "        if pd.notna(item['Choice_1']):\n",
    "            choices = []\n",
    "            choice_num = 1\n",
    "            input_text += \"\\n# Available Choices\\n\"\n",
    "            while True:\n",
    "                choice_key = f'Choice_{choice_num}'\n",
    "                if choice_key not in item or pd.isna(item[choice_key]):\n",
    "                    break\n",
    "                input_text += f\"- Choice {choice_num}: {item[choice_key]}\\n\"\n",
    "                choice_num += 1\n",
    "        \n",
    "        # Clear delimiter between input and expected output\n",
    "        input_text += \"\\n# Your Answer:\\n\"\n",
    "        \n",
    "        # Target output format\n",
    "        if pd.notna(item['Choice_1']):\n",
    "            correct_choice = item[f'Choice_{item[\"Correct_Answer\"]}']\n",
    "            target_text = (\n",
    "                f\"Answer: Choice {item['Correct_Answer']}. \"\n",
    "                f\"{correct_choice} is correct because it demonstrates the {item['Category'].lower()} \"\n",
    "                f\"concept. In the word '{item['Word']}', we can identify the {item['Task'].lower()} \"\n",
    "                f\"through proper morphological analysis. This is a key concept in understanding \"\n",
    "                f\"how words are formed and structured in English.\"\n",
    "            )\n",
    "        else:\n",
    "            target_text = (\n",
    "                f\"Answer: {str(item['Correct_Answer'])}. \"\n",
    "                f\"This demonstrates the {item['Category'].lower()} concept in '{item['Word']}'. \"\n",
    "                f\"When analyzing how this word {item['Task'].lower()}, we can see the morphological \"\n",
    "                f\"principles at work. This helps us understand the structure and formation of words.\"\n",
    "            )\n",
    "\n",
    "        # Combine input and target with EOS token\n",
    "        full_text = f\"{input_text}{target_text}</s>\"\n",
    "        \n",
    "        # Create encodings\n",
    "        encodings = self.tokenizer(\n",
    "            full_text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # Create labels with masked loss for prompt\n",
    "        input_only = self.tokenizer(\n",
    "            input_text,\n",
    "            add_special_tokens=False,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        input_length = input_only['input_ids'].shape[1]\n",
    "        labels = encodings['input_ids'].clone()\n",
    "        \n",
    "        # Set prompt part to -100 to ignore in loss calculation\n",
    "        labels[:, :input_length] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'].squeeze(),\n",
    "            'attention_mask': encodings['attention_mask'].squeeze(),\n",
    "            'labels': labels.squeeze()\n",
    "        }\n",
    "\n",
    "def prepare_data(csv_path):\n",
    "    \"\"\"Prepare and split the data for training with data augmentation\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # Convert necessary columns to string\n",
    "    for col in ['Correct_Answer', 'Word_Difficulty', 'Task_Difficulty']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Simple data augmentation: create small variations in questions\n",
    "    augmented_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        augmented_data.append(row.to_dict())  # Original row\n",
    "        \n",
    "        # Only augment if it's a multiple choice question\n",
    "        if pd.notna(row.get('Choice_1', pd.NA)):\n",
    "            # Variation 1: Slightly different instruction wording\n",
    "            variation = row.to_dict()\n",
    "            orig_instruction = variation['Instruction']\n",
    "            \n",
    "            if \"what is\" in orig_instruction.lower():\n",
    "                variation['Instruction'] = orig_instruction.lower().replace(\"what is\", \"identify\").capitalize()\n",
    "                augmented_data.append(variation)\n",
    "            elif \"identify\" in orig_instruction.lower():\n",
    "                variation['Instruction'] = orig_instruction.lower().replace(\"identify\", \"what is\").capitalize()\n",
    "                augmented_data.append(variation)\n",
    "    \n",
    "    # Convert back to DataFrame\n",
    "    augmented_df = pd.DataFrame(augmented_data)\n",
    "    \n",
    "    # Split data with stratification\n",
    "    try:\n",
    "        train_df, val_df = train_test_split(\n",
    "            augmented_df, \n",
    "            test_size=0.15,\n",
    "            random_state=42,\n",
    "            stratify=augmented_df[['Category', 'Task']].apply(lambda x: f\"{x['Category']}_{x['Task']}\", axis=1)\n",
    "        )\n",
    "    except ValueError:\n",
    "        # Fallback to stratifying by just Category\n",
    "        train_df, val_df = train_test_split(\n",
    "            augmented_df, \n",
    "            test_size=0.15, \n",
    "            random_state=42, \n",
    "            stratify=augmented_df['Category']\n",
    "        )\n",
    "    \n",
    "    print(f\"Original data size: {len(df)}\")\n",
    "    print(f\"Augmented data size: {len(augmented_df)}\")\n",
    "    print(f\"Training set size: {len(train_df)}, Validation set size: {len(val_df)}\")\n",
    "    \n",
    "    return train_df, val_df\n",
    "\n",
    "def monitor_gpu_memory(message):\n",
    "    \"\"\"Helper function to monitor GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / (1024 ** 3)\n",
    "        max_allocated = torch.cuda.max_memory_allocated() / (1024 ** 3)\n",
    "        reserved = torch.cuda.memory_reserved() / (1024 ** 3)\n",
    "        print(f\"{message}: Allocated: {allocated:.2f} GB, Max: {max_allocated:.2f} GB, Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "def save_config_locally(model_name, output_dir):\n",
    "    \"\"\"Save model config locally to avoid authentication issues during training\"\"\"\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    import shutil\n",
    "    \n",
    "    # Create directory structure\n",
    "    base_path = os.path.join(output_dir, \"base_model_config\")\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Download config file\n",
    "        config_path = hf_hub_download(\n",
    "            repo_id=model_name,\n",
    "            filename=\"config.json\",\n",
    "            token=HF_TOKEN\n",
    "        )\n",
    "        \n",
    "        # Copy to our directory\n",
    "        shutil.copy(config_path, os.path.join(base_path, \"config.json\"))\n",
    "        print(f\"Config saved locally to {base_path}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving config locally: {e}\")\n",
    "        return False\n",
    "\n",
    "def train_gemma_model(train_df, val_df, model_name=\"google/gemma-2b-it\", output_dir=\"gemma_morphology\", use_lora=True):\n",
    "    \"\"\"Train Gemma model with advanced techniques\"\"\"\n",
    "    # Clean memory\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    monitor_gpu_memory(\"Initial GPU state\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Save config locally to avoid authentication issues\n",
    "    save_config_locally(model_name, output_dir)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "    \n",
    "    # Save tokenizer locally to avoid authentication issues\n",
    "    tokenizer_save_path = os.path.join(output_dir, \"tokenizer\")\n",
    "    tokenizer.save_pretrained(tokenizer_save_path)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Load model with memory optimizations\n",
    "    print(\"Loading model...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        token=HF_TOKEN,\n",
    "        low_cpu_mem_usage=True,\n",
    "        torch_dtype=torch.float32,  # Always use float32 to avoid gradient issues\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    monitor_gpu_memory(\"After model loading\")\n",
    "    \n",
    "    # Apply LoRA for more efficient fine-tuning if requested\n",
    "    if use_lora:\n",
    "        print(\"Applying LoRA adapters...\")\n",
    "        # Configure LoRA\n",
    "        lora_config = LoraConfig(\n",
    "            r=16,  # rank\n",
    "            lora_alpha=32,  # scaling factor\n",
    "            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "            lora_dropout=0.05,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM\n",
    "        )\n",
    "        \n",
    "        # Apply LoRA to model\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        model.print_trainable_parameters()\n",
    "    else:\n",
    "        # Enable gradient checkpointing for full fine-tuning\n",
    "        model.gradient_checkpointing_enable(gradient_checkpointing_kwargs={\"use_reentrant\": False})\n",
    "    \n",
    "    monitor_gpu_memory(\"After model adaptation\")\n",
    "    \n",
    "    # Create datasets\n",
    "    print(\"Creating datasets...\")\n",
    "    train_dataset = MorphologyDataset(train_df, tokenizer, max_length=512)\n",
    "    val_dataset = MorphologyDataset(val_df, tokenizer, max_length=512)\n",
    "    \n",
    "    monitor_gpu_memory(\"After dataset creation\")\n",
    "\n",
    "    # Training arguments\n",
    "    batch_size = 2 if use_lora else 1\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=12,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_ratio=0.1,\n",
    "        learning_rate=3e-5 if use_lora else 2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=30,\n",
    "        save_steps=30,\n",
    "        save_total_limit=3,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        gradient_accumulation_steps=4 if use_lora else 8,\n",
    "        fp16=False,  # DISABLE fp16 to avoid gradient issues\n",
    "        bf16=False,\n",
    "        max_grad_norm=1.0,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        optim=\"adamw_torch\",\n",
    "        gradient_checkpointing=not use_lora,  # Enable for full fine-tuning only\n",
    "        ddp_find_unused_parameters=False,\n",
    "        dataloader_pin_memory=False,\n",
    "        report_to=\"none\",  # Disable reporting to save memory\n",
    "        run_name=f\"gemma_morphology_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n",
    "        hub_token=HF_TOKEN,  # Add token for Hugging Face API calls\n",
    "    )\n",
    "\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False\n",
    "    )\n",
    "    \n",
    "    monitor_gpu_memory(\"Before trainer initialization\")\n",
    "\n",
    "    # Initialize trainer with callbacks\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop if not improving\n",
    "    )\n",
    "    \n",
    "    monitor_gpu_memory(\"After trainer initialization\")\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    print(\"Saving model...\")\n",
    "    model_save_path = os.path.join(output_dir, \"final_model\")\n",
    "    \n",
    "    if use_lora:\n",
    "        # For LoRA, we save the adapter\n",
    "        model.save_pretrained(model_save_path, token=HF_TOKEN)\n",
    "    else:\n",
    "        # For full model, save everything\n",
    "        trainer.save_model(model_save_path)\n",
    "    \n",
    "    # Save tokenizer from the local copy to avoid authentication issues\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    \n",
    "    print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "def test_model(model, tokenizer, test_questions):\n",
    "    \"\"\"Test the model on a set of questions\"\"\"\n",
    "    model.eval()\n",
    "    results = []\n",
    "    \n",
    "    for question in test_questions:\n",
    "        # Structured prompt format\n",
    "        instruction = (\n",
    "            \"You are answering a morphology question. \"\n",
    "            \"Begin your answer with 'Answer: Choice X' for multiple choice or 'Answer: [text]' for open-ended questions. \"\n",
    "            \"Provide a clear explanation afterward.\"\n",
    "        )\n",
    "        \n",
    "        # Format input\n",
    "        input_text = (\n",
    "            f\"{instruction}\\n\\n\"\n",
    "            f\"# Question Information\\n\"\n",
    "            f\"- Type: Multiple choice\\n\"\n",
    "            f\"- Task: {question['Task']}\\n\"\n",
    "            f\"- Category: {question['Category']}\\n\"\n",
    "            f\"- Word: {question['Word']}\\n\"\n",
    "            f\"- Question: {question['Instruction']}\\n\"\n",
    "            f\"\\n# Available Choices\\n\"\n",
    "        )\n",
    "        \n",
    "        # Add choices\n",
    "        for i, choice in enumerate(question['Choices'], 1):\n",
    "            input_text += f\"- Choice {i}: {choice}\\n\"\n",
    "        \n",
    "        # Add answer prompt\n",
    "        input_text += \"\\n# Your Answer:\\n\"\n",
    "        \n",
    "        # Generation settings\n",
    "        generation_config = {\n",
    "            'max_new_tokens': 200,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.92,\n",
    "            'top_k': 50,\n",
    "            'repetition_penalty': 1.2,\n",
    "        }\n",
    "        \n",
    "        # Generate answer\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                **generation_config\n",
    "            )\n",
    "        \n",
    "        # Decode output\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract answer part\n",
    "        if input_text in generated_text:\n",
    "            answer = generated_text[len(input_text):].strip()\n",
    "        else:\n",
    "            answer = generated_text\n",
    "            \n",
    "        results.append({\n",
    "            'question': question,\n",
    "            'input': input_text,\n",
    "            'generated': answer\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    # Configure paths\n",
    "    data_path = 'Data/MC_Recoded.csv'\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    output_dir = f\"gemma_morphology_{timestamp}\"\n",
    "    \n",
    "    # Display GPU information\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB\")\n",
    "    \n",
    "    # Prepare data\n",
    "    print(\"\\nPreparing data...\")\n",
    "    train_df, val_df = prepare_data(data_path)\n",
    "    \n",
    "    # Define whether to use LoRA (recommended for better memory efficiency)\n",
    "    use_lora = True  # Set to False for full fine-tuning if you have enough GPU memory\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"\\nTraining Gemma model {'with LoRA' if use_lora else ''}...\")\n",
    "    model, tokenizer = train_gemma_model(train_df, val_df, output_dir=output_dir, use_lora=use_lora)\n",
    "    \n",
    "    # Test questions\n",
    "    test_questions = [\n",
    "        {\n",
    "            \"Task\": \"Identify\",\n",
    "            \"Category\": \"Derivation\",\n",
    "            \"Word\": \"happiness\",\n",
    "            \"Instruction\": \"What is the base word and suffix in 'happiness'?\",\n",
    "            \"Choices\": [\n",
    "                \"base: happy, suffix: -ness\",\n",
    "                \"base: happ, suffix: -iness\",\n",
    "                \"base: happi, suffix: -ness\",\n",
    "                \"base: hap, suffix: -piness\"\n",
    "            ]\n",
    "        },\n",
    "        {\n",
    "            \"Task\": \"Analyze\",\n",
    "            \"Category\": \"Compounding\",\n",
    "            \"Word\": \"blackboard\",\n",
    "            \"Instruction\": \"Identify the type of compound word in 'blackboard'.\",\n",
    "            \"Choices\": [\n",
    "                \"Endocentric compound\",\n",
    "                \"Exocentric compound\",\n",
    "                \"Copulative compound\",\n",
    "                \"Appositional compound\"\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Test the model\n",
    "    print(\"\\nTesting model on sample questions...\")\n",
    "    results = test_model(model, tokenizer, test_questions)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nSample Generated Answers:\")\n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"\\nExample {i}:\")\n",
    "        print(f\"Question: {result['question']['Instruction']}\")\n",
    "        print(f\"Word: {result['question']['Word']}\")\n",
    "        print(f\"Generated Answer: {result['generated']}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Clean memory before starting\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bf907-c59c-4a35-9e19-f5f7d0826360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-1.3",
   "language": "python",
   "name": "nlp-1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
